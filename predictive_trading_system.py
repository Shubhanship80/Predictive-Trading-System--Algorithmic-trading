# -*- coding: utf-8 -*-
"""Predictive Trading system.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tBaBPpmfzLroJZ-g4gLhRIxHVSkah7_C
"""

!pip install yfinance --upgrade --no-cache-dir

import yfinance as yf

msft = yf.Ticker("MSFT")
print(msft)

msft.info

#getting historiccal stock data
msft.history(period="max")

data = yf.download("SPY AAPL", start="2017-01-01", end="2017-04-30")

data.head(5)

import matplotlib.pyplot as plt

# Line plot of Closing Prices
plt.figure(figsize=(10, 5))
plt.plot(data['Adj Close']['AAPL'], label='AAPL')
plt.title('Stock Price - Closing Prices')
plt.xlabel('Date')
plt.ylabel('Closing Price')
plt.legend()
plt.show()

# Moving Average Plot
plt.figure(figsize=(10, 5))
data['Adj Close']['AAPL'].plot(label='AAPL', alpha=0.5)
data['Adj Close']['AAPL'].rolling(window=20).mean().plot(label='20-Day Moving Avg')
data['Adj Close']['AAPL'].rolling(window=50).mean().plot(label='50-Day Moving Avg')
plt.title('AAPL Stock Price with Moving Averages')
plt.xlabel('Date')
plt.ylabel('Stock Price')
plt.legend()
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report

# Create target variable
data['Target'] = (data['Close', 'AAPL'].shift(-1) > data['Close', 'AAPL']).astype(int)

#Define features (X) and target variable (y)
features = ['Open', 'High', 'Low', 'Close', 'Volume']
X = data[features]
y = data['Target']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = DecisionTreeClassifier(random_state=42)
model.fit(X_train, y_train)

predictions = model.predict(X_test)
# Evaluate the model
accuracy = accuracy_score(y_test, predictions)
print(f"Accuracy: {accuracy:.2f}")

# Display classification report
print("Classification Report:\n", classification_report(y_test, predictions))

"""Feature Engineering

"""

# Adding 5-day and 10-day moving averages
data['MA5_AAPL'] = data['Close', 'AAPL'].rolling(window=5).mean()
data['MA10_AAPL'] = data['Close', 'AAPL'].rolling(window=10).mean()

# Update features and target variable
features = ['Open', 'High', 'Low', 'Close', 'Volume', 'MA5_AAPL', 'MA10_AAPL']
X = data[features]
# Drop rows with NaN values introduced by the moving averages calculation
X = X.dropna()
y = data['Target'].loc[X.index]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""Hyperparameter Tuning"""

from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
# Define hyperparameters to search
param_grid = {'max_depth': [None, 5, 10, 15],
              'min_samples_split': [2, 5, 10],
              'min_samples_leaf': [1, 2, 4]}

# Create the GridSearchCV object
grid_search = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=5)

# Fit the model with the best hyperparameters
grid_search.fit(X_train, y_train)

# Get the best model
best_model = grid_search.best_estimator_

# Make predictions on the test set
predictions = best_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, predictions)
print(f"Accuracy with optimized hyperparameters: {accuracy:.2f}")

"""Ensemble method using Random Forest

"""

from sklearn.ensemble import RandomForestClassifier

# Create and train the Random Forest model
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train, y_train)

# Make predictions on the test set
rf_predictions = rf_model.predict(X_test)

# Evaluate the Random Forest model
rf_accuracy = accuracy_score(y_test, rf_predictions)
print(f"Random Forest Accuracy: {rf_accuracy:.2f}")

from sklearn.metrics import classification_report

# Obtain the classification report for Random Forest model
rf_classification_report = classification_report(y_test, rf_predictions)
print("Random Forest Classification Report:\n", rf_classification_report)

"""Time-series cross-validation Analysis

"""

from sklearn.model_selection import TimeSeriesSplit

# Create TimeSeriesSplit object
tscv = TimeSeriesSplit(n_splits=5)

# Use it in GridSearchCV
grid_search = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=tscv)

# Fit the model with the best hyperparameters
grid_search.fit(X_train, y_train)

# Get the best model
best_model = grid_search.best_estimator_

# Make predictions on the test set
predictions = best_model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, predictions)
print(f"Accuracy with time series cross-validation: {accuracy:.2f}")

"""RNNs and LSTM models"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from sklearn.preprocessing import MinMaxScaler

# Assuming 'data' is your DataFrame
# Assuming you have already created a target variable 'Target' indicating the price movement (1 for up, 0 for down)

# Scale the features
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(data['Close', 'AAPL'].values.reshape(-1, 1))

# Create a function to prepare the data for the LSTM model
def create_sequences(data, seq_length):
    sequences, targets = [], []
    for i in range(len(data) - seq_length):
        seq = data[i:i+seq_length]
        target = data[i+seq_length]
        sequences.append(seq)
        targets.append(target)
    return np.array(sequences), np.array(targets)

# Define the sequence length
sequence_length = 10

# Prepare the data
X, y = create_sequences(scaled_data, sequence_length)

# Split the data into training and testing sets
split = int(0.8 * len(X))
X_train, X_test, y_train, y_test = X[:split], X[split:], y[:split], y[split:]

# Build the LSTM model
model = Sequential()
model.add(LSTM(units=50, activation='relu', input_shape=(X_train.shape[1], 1)))
model.add(Dense(units=1))
model.compile(optimizer='adam', loss='mean_squared_error')

# Train the model
model.fit(X_train, y_train, epochs=50, batch_size=32)

# Make predictions on the test set
predictions = model.predict(X_test)

# Inverse transform the predictions to get back the original scale
predictions = scaler.inverse_transform(predictions)

from sklearn.metrics import mean_squared_error, mean_absolute_error

# Assuming 'y_test' is your true labels and 'predictions' is the predicted values for LSTM model

# Inverse transform the true labels to get back the original scale
y_test_original = scaler.inverse_transform(y_test.reshape(-1, 1))

# Inverse transform the predictions to get back the original scale
predictions_original = scaler.inverse_transform(predictions)

# Mean Squared Error (MSE)
mse = mean_squared_error(y_test_original, predictions_original)
print(f"Mean Squared Error (MSE): {mse:.2f}")

# Mean Absolute Error (MAE)
mae = mean_absolute_error(y_test_original, predictions_original)
print(f"Mean Absolute Error (MAE): {mae:.2f}")

# Root Mean Squared Error (RMSE)
rmse = np.sqrt(mse)
print(f"Root Mean Squared Error (RMSE): {rmse:.2f}")

import matplotlib.pyplot as plt

# Assuming 'y_test_original' is your true labels and 'predictions_original' is the predicted values

# Plot true values
plt.plot(y_test_original, label='True Values', color='blue')

# Plot predicted values
plt.plot(predictions_original, label='Predicted Values', color='red')

# Set labels and title
plt.xlabel('Time')
plt.ylabel('Price')
plt.title('True vs Predicted Values')

# Show legend
plt.legend()

# Show the plot
plt.show()

"""Improving the model performance"""

model = Sequential()
model.add(LSTM(units=100, activation='relu', input_shape=(X_train.shape[1], 1), return_sequences=True))
model.add(LSTM(units=50, activation='relu'))
model.add(Dense(units=1))
model.compile(optimizer='adam', loss='mean_squared_error')

# Adjust the sequence length
sequence_length = 20
X, y = create_sequences(scaled_data, sequence_length)

#hyperparameter tuning
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mean_squared_error')
model.fit(X_train, y_train, epochs=100, batch_size=64)

# Assuming 'y_test_original' is your true labels and 'predictions_original' is the predicted values

# Mean Squared Error (MSE)
mse = mean_squared_error(y_test_original, predictions_original)
print(f"Mean Squared Error (MSE): {mse:.2f}")

# Mean Absolute Error (MAE)
mae = mean_absolute_error(y_test_original, predictions_original)
print(f"Mean Absolute Error (MAE): {mae:.2f}")

# Root Mean Squared Error (RMSE)
rmse = np.sqrt(mse)
print(f"Root Mean Squared Error (RMSE): {rmse:.2f}")